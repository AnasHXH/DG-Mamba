{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FSovHjCR2Q35",
        "outputId": "e7f0e501-f714-4794-9feb-f493b1080513"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision einops==0.3.2 tqdm==4.64.1 numpy==1.23.5 scikit-image==0.19.3 lpips==0.1.4 pyyaml==6.0 Pillow==9.4.0 matplotlib==3.6.2 opencv-python==4.6.0.66\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2XGRjOI5XOB"
      },
      "source": [
        "## Instructions to Access and Use the Datasets\n",
        "\n",
        "Open the provided dataset links:\n",
        "\n",
        "iSAID Dataset: https://drive.google.com/file/d/1mlTTdbqG1ZheaWsBcIjAKDyCdbuAqpvy/view\n",
        "\n",
        "darkrs Dataset : https://drive.google.com/file/d/1XQGpzB9vDGkO7ULnGOF86cyZdqtrX4tI/view\n",
        "\n",
        "LOL Dataset : https://drive.google.com/file/d/1L-kqSQyrmMueBh_ziWoPFhfsAh50h20H/view\n",
        "\n",
        "Exdark Dataset: https://github.com/cs-chan/Exclusively-Dark-Image-Dataset?tab=readme-ov-file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2zXsw912ocI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "from torchvision.utils import save_image\n",
        "from einops import rearrange\n",
        "from PIL import Image\n",
        "import os\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "# Constants\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import random\n",
        "\n",
        "class ShadowRemovalDataset(Dataset):\n",
        "    def __init__(self, gt_dir, lq_dir, transform=None, augment=False):\n",
        "        self.gt_dir = gt_dir\n",
        "        self.lq_dir = lq_dir\n",
        "        self.transform = transform\n",
        "        self.augment = augment\n",
        "        self.gt_images = sorted(os.listdir(gt_dir))\n",
        "        self.lq_images = sorted(os.listdir(lq_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.gt_images)\n",
        "\n",
        "    def augment_images(self, gt_image, lq_image):\n",
        "        # Random rotation\n",
        "        angle = random.choice([0, 90, 180, 270])\n",
        "        gt_image = TF.rotate(gt_image, angle)\n",
        "        lq_image = TF.rotate(lq_image, angle)\n",
        "\n",
        "        # Random horizontal flipping\n",
        "        if random.random() > 0.25:\n",
        "            gt_image = TF.hflip(gt_image)\n",
        "            lq_image = TF.hflip(lq_image)\n",
        "\n",
        "        # Random vertical flipping\n",
        "        if random.random() > 0.25:\n",
        "            gt_image = TF.vflip(gt_image)\n",
        "            lq_image = TF.vflip(lq_image)\n",
        "\n",
        "        return gt_image, lq_image\n",
        "\n",
        "    def random_crop(self, gt_image, lq_image):\n",
        "        i, j, h, w = transforms.RandomCrop.get_params(\n",
        "            gt_image, output_size=(256, 256))\n",
        "        gt_image = TF.crop(gt_image, i, j, h, w)\n",
        "        lq_image = TF.crop(lq_image, i, j, h, w)\n",
        "        return gt_image, lq_image\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        gt_img_path = os.path.join(self.gt_dir, self.gt_images[idx])\n",
        "        lq_img_path = os.path.join(self.lq_dir, self.lq_images[idx])\n",
        "\n",
        "        gt_image = Image.open(gt_img_path).convert(\"RGB\")\n",
        "        lq_image = Image.open(lq_img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.augment:\n",
        "            #pass\n",
        "            gt_image, lq_image = self.augment_images(gt_image, lq_image)\n",
        "            gt_image, lq_image = self.random_crop(gt_image, lq_image)\n",
        "\n",
        "        if self.transform:\n",
        "            gt_image = self.transform(gt_image)\n",
        "            lq_image = self.transform(lq_image)\n",
        "\n",
        "        return lq_image, gt_image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SD_3Dsiy3C7F"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleGate(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x1, x2 = x.chunk(2, dim=-1)\n",
        "        return x1 * x2\n",
        "\n",
        "class ASPP(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ASPP, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(in_channels, out_channels, 3, padding=6, dilation=6, bias=False)\n",
        "        self.conv3 = nn.Conv2d(in_channels, out_channels, 3, padding=12, dilation=12, bias=False)\n",
        "        self.conv4 = nn.Conv2d(in_channels, out_channels, 3, padding=18, dilation=18, bias=False)\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.conv5 = nn.Conv2d(in_channels, out_channels, 1, bias=False)\n",
        "        self.conv_out = nn.Conv2d(out_channels * 5, out_channels, 1, bias=False)\n",
        "        self.norm = nn.LayerNorm(out_channels)\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        size = x.shape[-2:]\n",
        "        feat1 = self.conv1(x)\n",
        "        feat2 = self.conv2(x)\n",
        "        feat3 = self.conv3(x)\n",
        "        feat4 = self.conv4(x)\n",
        "        feat5 = torch.nn.functional.interpolate(self.conv5(self.pool(x)), size=size, mode='bilinear', align_corners=False)\n",
        "        out = torch.cat((feat1, feat2, feat3, feat4, feat5), dim=1)\n",
        "        out = self.conv_out(out)\n",
        "        out = out.permute(0, 2, 3, 1)  # Change to (B, H, W, C)\n",
        "        out = self.norm(out)\n",
        "        out = out.permute(0, 3, 1, 2)  # Change back to (B, C, H, W)\n",
        "        return self.act(out)\n",
        "\n",
        "class CWSA(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super(CWSA, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.query_conv = nn.Linear(dim, dim)\n",
        "        self.key_conv = nn.Linear(dim, dim)\n",
        "        self.value_conv = nn.Linear(dim, dim)\n",
        "        self.scale = dim ** -0.5\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, 1, 1, dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, H, W, C)\n",
        "        B, H, W, C = x.shape\n",
        "        x = x + self.pos_embedding  # Positional embedding\n",
        "        x = x.view(B, H * W, C)     # Reshape to (B, N, C)\n",
        "\n",
        "        # Linear projections\n",
        "        q = self.query_conv(x)      # (B, N, C)\n",
        "        k = self.key_conv(x)        # (B, N, C)\n",
        "        v = self.value_conv(x)      # (B, N, C)\n",
        "\n",
        "        # Compute attention over channels at each spatial location\n",
        "        q = q.view(B, H * W, 1, C)  # (B, N, 1, C)\n",
        "        k = k.view(B, H * W, C, 1)  # (B, N, C, 1)\n",
        "        attn = torch.matmul(q, k).squeeze(2) * self.scale  # (B, N, C)\n",
        "        attn = attn.softmax(dim=-1)  # Softmax over channels\n",
        "\n",
        "        # Apply attention to values\n",
        "        out = attn * v               # Element-wise multiplication\n",
        "        out = out.view(B, H, W, C)   # Reshape back to (B, H, W, C)\n",
        "        return out\n",
        "\n",
        "class SSVAModule(nn.Module):\n",
        "    def __init__(self, d_model, d_state=16, d_conv=3, expand=2.0, dt_rank=64, dt_min=0.001, dt_max=0.1, dt_init=\"random\", dt_scale=1.0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_state = d_state\n",
        "        self.d_conv = d_conv\n",
        "        self.expand = expand\n",
        "        self.d_inner = int(self.expand * self.d_model)  # self.d_inner = 2 * d_model\n",
        "        self.dt_rank = dt_rank\n",
        "\n",
        "        self.in_proj = nn.Linear(self.d_model, self.d_inner * 2)\n",
        "        self.conv2d = nn.Conv2d(self.d_inner, self.d_inner, kernel_size=d_conv, padding=(d_conv - 1) // 2, groups=self.d_inner)\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "        self.x_proj = nn.Linear(self.d_inner, self.d_inner * 2)\n",
        "        self.dt_proj = nn.Linear(self.d_inner, self.d_inner)\n",
        "\n",
        "        self.out_norm = nn.LayerNorm(self.d_inner)\n",
        "\n",
        "        self.out_proj = nn.Linear(self.d_inner // 2, d_model)\n",
        "\n",
        "        # New components\n",
        "        self.simple_gate = SimpleGate()\n",
        "        self.aspp = ASPP(d_model, d_model)\n",
        "        self.channel_attn = CWSA(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, H, W, C = x.shape\n",
        "\n",
        "        # Apply ASPP\n",
        "        x_aspp = self.aspp(x.permute(0, 3, 1, 2)).permute(0, 2, 3, 1)\n",
        "\n",
        "        # Original SS2D operations\n",
        "        x = self.in_proj(x)\n",
        "        x, z = x.chunk(2, dim=-1)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        x = self.conv2d(x)\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = self.act(x)\n",
        "        y = self.selective_scan(x)\n",
        "        y = self.out_norm(y)\n",
        "        y = y * F.silu(z)\n",
        "\n",
        "        # Apply SimpleGate\n",
        "        y = self.simple_gate(y)\n",
        "\n",
        "        # Apply Channel-wise Self-Attention\n",
        "        y = self.channel_attn(y)\n",
        "\n",
        "        # Combine with ASPP output\n",
        "        y = y + x_aspp\n",
        "\n",
        "        out = self.out_proj(y)\n",
        "        return out\n",
        "\n",
        "    def selective_scan(self, x):\n",
        "        B, H, W, C = x.shape\n",
        "        x_flat = x.reshape(B, H * W, C)\n",
        "        x_dbl = self.x_proj(x_flat)\n",
        "        x_dbl = x_dbl.view(B, H, W, -1)\n",
        "        dt, x_proj = x_dbl.chunk(2, dim=-1)\n",
        "        dt = F.softplus(self.dt_proj(dt))\n",
        "        y = x * torch.sigmoid(dt) + x_proj * torch.tanh(x_proj)\n",
        "        return y\n",
        "\n",
        "class SSVABlock(nn.Module):\n",
        "    def __init__(self, d_model, d_state=16):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(d_model)\n",
        "        self.ss2d = SSVAModule(d_model, d_state)\n",
        "        self.ln_2 = nn.LayerNorm(d_model)\n",
        "        self.conv_blk = nn.Sequential(\n",
        "            nn.Conv2d(d_model, d_model, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(d_model, d_model, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.ln_1(x)\n",
        "        x = residual + self.ss2d(x)\n",
        "        residual = x\n",
        "        x = self.ln_2(x)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        x = self.conv_blk(x)\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = residual + x\n",
        "        return x\n",
        "\n",
        "class DBRSNet(nn.Module):\n",
        "    def __init__(self, img_channel=3, width=32, middle_blk_num=1, enc_blk_nums=[1, 1, 1, 1], dec_blk_nums=[1, 1, 1, 1], d_state=64):\n",
        "        super().__init__()\n",
        "        self.intro = nn.Conv2d(img_channel, width, kernel_size=3, padding=1, stride=1, groups=1, bias=True)\n",
        "        self.ending = nn.Conv2d(width, img_channel, kernel_size=3, padding=1, stride=1, groups=1, bias=True)\n",
        "\n",
        "        self.encoders = nn.ModuleList()\n",
        "        self.decoders = nn.ModuleList()\n",
        "        self.middle_blks = nn.ModuleList()\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "\n",
        "        chan = width\n",
        "        for num in enc_blk_nums:\n",
        "            self.encoders.append(\n",
        "                nn.Sequential(*[SSVABlock(chan, d_state) for _ in range(num)])\n",
        "            )\n",
        "            self.downs.append(nn.Conv2d(chan, 2 * chan, 2, 2))\n",
        "            chan = chan * 2\n",
        "\n",
        "        self.middle_blks = nn.Sequential(\n",
        "            *[SSVABlock(chan, d_state) for _ in range(middle_blk_num)]\n",
        "        )\n",
        "\n",
        "        for num in dec_blk_nums:\n",
        "            self.ups.append(nn.Sequential(\n",
        "                nn.Conv2d(chan, chan * 2, 1, bias=False),\n",
        "                nn.PixelShuffle(2)\n",
        "            ))\n",
        "            chan = chan // 2\n",
        "            self.decoders.append(\n",
        "                nn.Sequential(*[SSVABlock(chan, d_state) for _ in range(num)])\n",
        "            )\n",
        "\n",
        "        self.padder_size = 2 ** len(self.encoders)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        B, C, H, W = inp.shape\n",
        "        inp = self.check_image_size(inp)\n",
        "        x = self.intro(inp)\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "\n",
        "        encs = []\n",
        "        for encoder, down in zip(self.encoders, self.downs):\n",
        "            x = encoder(x)\n",
        "            encs.append(x)\n",
        "            x = x.permute(0, 3, 1, 2)\n",
        "            x = down(x)\n",
        "            x = x.permute(0, 2, 3, 1)\n",
        "\n",
        "        x = self.middle_blks(x)\n",
        "\n",
        "        for decoder, up, enc_skip in zip(self.decoders, self.ups, encs[::-1]):\n",
        "            x = x.permute(0, 3, 1, 2)\n",
        "            x = up(x)\n",
        "            x = x.permute(0, 2, 3, 1)\n",
        "            x = x + enc_skip\n",
        "            x = decoder(x)\n",
        "\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        x = self.ending(x)\n",
        "        x = x + inp\n",
        "\n",
        "        return x[:, :, :H, :W]\n",
        "\n",
        "    def check_image_size(self, x):\n",
        "        _, _, h, w = x.size()\n",
        "        mod_pad_h = (self.padder_size - h % self.padder_size) % self.padder_size\n",
        "        mod_pad_w = (self.padder_size - w % self.padder_size) % self.padder_size\n",
        "        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h))\n",
        "        return x\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DualBranch_DBRSNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_channel=3,\n",
        "        width=32,\n",
        "        middle_blk_num=1,\n",
        "        enc_blk_nums=[1, 1, 1, 1],\n",
        "        dec_blk_nums=[1, 1, 1, 1],\n",
        "        d_state=64\n",
        "    ):\n",
        "        \"\"\"\n",
        "        A simple dual-branch architecture that internally instantiates two\n",
        "        'MambaIRShadowRemoval' submodules, then fuses their outputs.\n",
        "\n",
        "        Args:\n",
        "            img_channel: number of input image channels\n",
        "            width: base channel width for each U-Net branch\n",
        "            middle_blk_num: number of middle blocks in each branch\n",
        "            enc_blk_nums: number of blocks in each encoder stage\n",
        "            dec_blk_nums: number of blocks in each decoder stage\n",
        "            d_state: hidden dimension for the EnhancedSS2D blocks\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Branch A\n",
        "        self.branch_a = DBRSNet(\n",
        "            img_channel=img_channel,\n",
        "            width=width,\n",
        "            middle_blk_num=middle_blk_num,\n",
        "            enc_blk_nums=enc_blk_nums,\n",
        "            dec_blk_nums=dec_blk_nums,\n",
        "            d_state=d_state\n",
        "        )\n",
        "\n",
        "        # Branch B (can have the same or different hyperparameters)\n",
        "        self.branch_b = DBRSNet(\n",
        "            img_channel=img_channel,\n",
        "            width=width,\n",
        "            middle_blk_num=middle_blk_num,\n",
        "            enc_blk_nums=enc_blk_nums,\n",
        "            dec_blk_nums=dec_blk_nums,\n",
        "            d_state=d_state\n",
        "        )\n",
        "\n",
        "        # Example fusion layer (you can replace this with something fancier)\n",
        "        # Here we just do a simple 1x1 conv after concatenation\n",
        "        # If you just want an additive fusion, you can do out = outA + outB\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Conv2d(img_channel * 2, img_channel, kernel_size=1, stride=1),\n",
        "            nn.Sigmoid()  # or any other activation\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        If you have two different inputs, x1 and x2, you can:\n",
        "            out_a = self.branch_a(x1)\n",
        "            out_b = self.branch_b(x2)\n",
        "        For demonstration, here we assume a single input that both branches see.\n",
        "        \"\"\"\n",
        "        out_a = self.branch_a(x)  # (B, C, H, W)\n",
        "        out_b = self.branch_b(x)  # (B, C, H, W)\n",
        "\n",
        "        # Simple example: fuse by concatenation + 1x1 conv\n",
        "        fused = torch.cat([out_a, out_b], dim=1)  # (B, 2*C, H, W)\n",
        "        fused = self.fusion(fused)               # (B, C, H, W)\n",
        "\n",
        "        return fused\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUiQVPES6PX0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Loss functions (unchanged)\n",
        "class VGGLoss(nn.Module):\n",
        "    def __init__(self, device=DEVICE):\n",
        "        super(VGGLoss, self).__init__()\n",
        "        vgg = models.vgg19(pretrained=True).features.to(device).eval()\n",
        "        self.vgg_layers = nn.Sequential(*list(vgg)[:36]).to(device)\n",
        "        self.device = device\n",
        "        for param in self.vgg_layers.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = x.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "        x_vgg = self.vgg_layers(x)\n",
        "        y_vgg = self.vgg_layers(y)\n",
        "        return nn.functional.l1_loss(x_vgg, y_vgg)\n",
        "\n",
        "class FocalFrequencyLoss(nn.Module):\n",
        "    def __init__(self, alpha=1.0, device=DEVICE):\n",
        "        super(FocalFrequencyLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        input = input.to(self.device)\n",
        "        target = target.to(self.device)\n",
        "        input_fft = torch.fft.fft2(input)\n",
        "        target_fft = torch.fft.fft2(target)\n",
        "        diff = input_fft - target_fft\n",
        "        abs_diff = torch.abs(diff)\n",
        "        loss = torch.pow(abs_diff, self.alpha)\n",
        "        return torch.mean(loss)\n",
        "\n",
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, lambda_vgg=0.01, lambda_ff=0.1, device=DEVICE):\n",
        "        super(CombinedLoss, self).__init__()\n",
        "        self.lambda_vgg = lambda_vgg\n",
        "        self.lambda_ff = lambda_ff\n",
        "        self.l1_loss = nn.L1Loss()\n",
        "        self.vgg_loss = VGGLoss(device=device)\n",
        "        self.ff_loss = FocalFrequencyLoss(device=device)\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        l1 = self.l1_loss(input, target)\n",
        "        vgg = self.vgg_loss(input, target)\n",
        "        ff = self.ff_loss(input, target)\n",
        "        return l1 + self.lambda_vgg * vgg + self.lambda_ff * ff\n",
        "\n",
        "class VGG_comLoss(nn.Module):\n",
        "    def __init__(self, lambda_vgg=0.01, lambda_ff=0.1, device=DEVICE):\n",
        "        super(VGG_comLoss, self).__init__()\n",
        "        self.lambda_vgg = lambda_vgg\n",
        "        self.lambda_ff = lambda_ff\n",
        "        self.l1_loss = nn.L1Loss()\n",
        "        self.vgg_loss = VGGLoss(device=device)\n",
        "        self.ff_loss = FocalFrequencyLoss(device=device)\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        l1 = self.l1_loss(input, target)\n",
        "        vgg = self.vgg_loss(input, target)\n",
        "        ff = self.ff_loss(input, target)\n",
        "        return l1 + self.lambda_vgg * vgg\n",
        "\n",
        "class CharbonnierLoss(nn.Module):\n",
        "    \"\"\"Charbonnier Loss (L1)\"\"\"\n",
        "\n",
        "    def __init__(self, eps=1e-3):\n",
        "        super(CharbonnierLoss, self).__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        diff = x - y\n",
        "        # loss = torch.sum(torch.sqrt(diff * diff + self.eps))\n",
        "        loss = torch.mean(torch.sqrt((diff * diff) + (self.eps*self.eps)))\n",
        "        return loss\n",
        "# l1_loss = nn.L1Loss()\n",
        "# Training and validation functions (unchanged)\n",
        "\n",
        "\n",
        "\n",
        "def train(model, train_loader, val_loader, num_epochs, device, save_dir, LEARNING_RATE):\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999))\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-8)\n",
        "    criterion = VGG_comLoss(device= device)\n",
        "\n",
        "    # Initialize lists to store metrics\n",
        "    training_losses = []\n",
        "    validation_losses = []\n",
        "    psnr_list = []\n",
        "    ssim_list = []\n",
        "    lpips_list = []\n",
        "    psnr_basicsr_list = []\n",
        "    ssim_basicsr_list = []\n",
        "\n",
        "    # Initialize best PSNR\n",
        "    best_psnr = 0.0\n",
        "    # best_lpips = 100.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{NUM_EPOCHS}', unit='batch') as pbar:\n",
        "            for real_shadow, real_free in train_loader:\n",
        "                real_shadow, real_free = real_shadow.to(device), real_free.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                pred_free = model(real_shadow)\n",
        "                loss = criterion(pred_free, real_free)\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                pbar.set_postfix(loss=loss.item())\n",
        "                pbar.update(1)\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        training_losses.append(avg_loss)\n",
        "        print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}] - Average Loss: {avg_loss:.4f}\")\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validate every epoch\n",
        "        avg_val_loss, avg_psnr, avg_ssim, avg_lpips, avg_psnr_basicsr, avg_ssim_basicsr = validate_and_save(\n",
        "            model, val_loader, criterion, device, epoch, save_dir, num_epochs\n",
        "        )\n",
        "\n",
        "        validation_losses.append(avg_val_loss)\n",
        "        psnr_list.append(avg_psnr)\n",
        "        ssim_list.append(avg_ssim)\n",
        "        lpips_list.append(avg_lpips)\n",
        "        psnr_basicsr_list.append(avg_psnr_basicsr)\n",
        "        ssim_basicsr_list.append(avg_ssim_basicsr)\n",
        "\n",
        "        # Check if current avg_psnr is better than best_psnr\n",
        "        if avg_psnr > best_psnr:\n",
        "        # if avg_lpips < best_lpips:\n",
        "            best_psnr = avg_psnr\n",
        "            # best_lpips = avg_lpips\n",
        "            # Save model checkpoint\n",
        "            checkpoint_dir = os.path.join(save_dir, 'best_checkpoints')\n",
        "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "            checkpoint_path = os.path.join(checkpoint_dir, f'shadow_removal_best.pth')\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            print(f\"New best PSNR: {best_psnr:.4f}. Model saved to {checkpoint_path}\")\n",
        "        else:\n",
        "            print(f\"PSNR did not improve from {best_psnr:.4f}\")\n",
        "\n",
        "\n",
        "    # After training, save metrics to CSV\n",
        "    import pandas as pd\n",
        "    metrics_data = {\n",
        "        'Epoch': list(range(1, num_epochs + 1)),\n",
        "        'Training Loss': training_losses,\n",
        "        'Validation Loss': validation_losses,\n",
        "        'PSNR': psnr_list,\n",
        "        'SSIM': ssim_list,\n",
        "        'LPIPS': lpips_list,\n",
        "        'PSNR Basicsr': psnr_basicsr_list,\n",
        "        'SSIM Basicsr': ssim_basicsr_list\n",
        "    }\n",
        "    metrics_df = pd.DataFrame(metrics_data)\n",
        "    csv_path = os.path.join(save_dir, 'training_metrics.csv')\n",
        "    metrics_df.to_csv(csv_path, index=False)\n",
        "    print(f\"Metrics saved to {csv_path}\")\n",
        "\n",
        "    # Plot the curves\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Plot Training and Validation Loss\n",
        "    plt.figure()\n",
        "    plt.plot(metrics_df['Epoch'], metrics_df['Training Loss'], label='Training Loss')\n",
        "    plt.plot(metrics_df['Epoch'], metrics_df['Validation Loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(save_dir, 'loss_curve.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Plot PSNR\n",
        "    plt.figure()\n",
        "    plt.plot(metrics_df['Epoch'], metrics_df['PSNR'], label='PSNR (RGB)')\n",
        "    plt.plot(metrics_df['Epoch'], metrics_df['PSNR Basicsr'], label='PSNR Basicsr (Y channel)')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('PSNR')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(save_dir, 'psnr_curve.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Plot SSIM\n",
        "    plt.figure()\n",
        "    plt.plot(metrics_df['Epoch'], metrics_df['SSIM'], label='SSIM (RGB)')\n",
        "    plt.plot(metrics_df['Epoch'], metrics_df['SSIM Basicsr'], label='SSIM Basicsr (Y channel)')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('SSIM')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(save_dir, 'ssim_curve.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Plot LPIPS\n",
        "    plt.figure()\n",
        "    plt.plot(metrics_df['Epoch'], metrics_df['LPIPS'], label='LPIPS')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('LPIPS')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(save_dir, 'lpips_curve.png'))\n",
        "    plt.close()\n",
        "\n",
        "    print(\"Training completed\")\n",
        "\n",
        "\n",
        "####################################################\n",
        "# Validation\n",
        "# %cd /data/Image_restoration/shadow_removal_cvpr_2025/my_model/window/BasicSR\n",
        "import lpips\n",
        "import torch\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "# from basicsr.metrics.psnr_ssim import calculate_psnr_pt, calculate_ssim_pt\n",
        "# from basicsr.utils import img2tensor\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import numpy as np\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "def validate_and_save(model, val_loader, criterion, device, epoch, save_dir, num_epochs):\n",
        "    model.eval()\n",
        "    total_psnr = 0\n",
        "    total_ssim = 0\n",
        "    total_psnr_basicsr = 0\n",
        "    total_ssim_basicsr = 0\n",
        "    total_lpips = 0\n",
        "    val_loss = 0\n",
        "    total_num_images = 0  # Keep track of total images processed\n",
        "\n",
        "    # Initialize LPIPS model\n",
        "    lpips_model = lpips.LPIPS(net='alex').to(device)\n",
        "\n",
        "    # Create a separate folder for checkpoints\n",
        "    checkpoint_dir = os.path.join(save_dir, 'checkpoints')\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    sample_images_dir = os.path.join(save_dir, 'sample_images')\n",
        "    os.makedirs(sample_images_dir, exist_ok=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (real_shadow, real_free) in enumerate(val_loader):\n",
        "            real_shadow, real_free = real_shadow.to(device), real_free.to(device)\n",
        "            pred_free = model(real_shadow)\n",
        "\n",
        "            loss = criterion(pred_free, real_free)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            batch_size = real_free.size(0)\n",
        "            total_num_images += batch_size\n",
        "\n",
        "            # Calculate PSNR and SSIM using original method (on RGB images)\n",
        "            pred_free_np = pred_free.cpu().numpy().transpose(0, 2, 3, 1)\n",
        "            real_free_np = real_free.cpu().numpy().transpose(0, 2, 3, 1)\n",
        "\n",
        "            for j in range(batch_size):\n",
        "                # Original PSNR and SSIM (RGB images)\n",
        "                psnr_value = psnr(real_free_np[j], pred_free_np[j], data_range=1)\n",
        "                ssim_value = ssim(real_free_np[j], pred_free_np[j], data_range=1, channel_axis=-1)\n",
        "                total_psnr += psnr_value\n",
        "                total_ssim += ssim_value\n",
        "\n",
        "            # Calculate PSNR and SSIM using basicsr (on tensors)\n",
        "            psnr_basicsr = calculate_psnr_pt(pred_free, real_free, crop_border=4, test_y_channel=True)\n",
        "            ssim_basicsr = calculate_ssim_pt(pred_free, real_free, crop_border=4, test_y_channel=True)\n",
        "            total_psnr_basicsr += psnr_basicsr.sum().item()\n",
        "            total_ssim_basicsr += ssim_basicsr.sum().item()\n",
        "\n",
        "            # Calculate LPIPS\n",
        "            lpips_score = lpips_model(pred_free, real_free)\n",
        "            total_lpips += lpips_score.sum().item()\n",
        "\n",
        "            # Save some sample images\n",
        "            if i == 0:\n",
        "                save_image(pred_free, os.path.join(sample_images_dir, f'pred_free_epoch_{epoch + 1}.png'))\n",
        "                save_image(real_shadow, os.path.join(sample_images_dir, f'real_shadow_epoch_{epoch + 1}.png'))\n",
        "                save_image(real_free, os.path.join(sample_images_dir, f'real_free_epoch_{epoch + 1}.png'))\n",
        "\n",
        "    avg_psnr = total_psnr / total_num_images\n",
        "    avg_ssim = total_ssim / total_num_images\n",
        "    avg_psnr_basicsr = total_psnr_basicsr / total_num_images\n",
        "    avg_ssim_basicsr = total_ssim_basicsr / total_num_images\n",
        "    avg_lpips = total_lpips / total_num_images\n",
        "    avg_loss = val_loss / len(val_loader)\n",
        "\n",
        "    print(f\"Validation after Epoch [{epoch + 1}/{num_epochs}]\")\n",
        "    print(f\"Avg Loss: {avg_loss:.4f}\")\n",
        "    print(f\"Original PSNR (RGB): {avg_psnr:.4f}, Original SSIM (RGB): {avg_ssim:.4f}\")\n",
        "    print(f\"Basicsr PSNR (Y channel, crop=4): {avg_psnr_basicsr:.4f}, Basicsr SSIM (Y channel, crop=4): {avg_ssim_basicsr:.4f}\")\n",
        "    print(f\"Avg LPIPS: {avg_lpips:.4f}\")\n",
        "\n",
        "    # Save model checkpoint in the separate folder\n",
        "    #checkpoint_path = os.path.join(checkpoint_dir, f'shadow_removal_epoch_{epoch + 1}.pth')\n",
        "    #torch.save(model.state_dict(), checkpoint_path)\n",
        "    #print(f\"Checkpoint saved to {checkpoint_path}\")\n",
        "\n",
        "    return avg_loss, avg_psnr, avg_ssim, avg_lpips, avg_psnr_basicsr, avg_ssim_basicsr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BarlqUux6eY6"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fhue4BWm6ZSL"
      },
      "outputs": [],
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Define directories\n",
        "    LEARNING_RATE = 0.0002\n",
        "    NUM_EPOCHS = 500\n",
        "    BATCH_SIZE = 1\n",
        "    SAVE_INTERVAL = 10\n",
        "\n",
        "    train_gt_dir = '/home/riotu/Image_restoration/Hamad/copyright/dataset/iSAID-dataset/train/'\n",
        "    train_lq_dir = '/home/riotu/Image_restoration/Hamad/copyright/dataset/iSAID-dataset/train/low'\n",
        "    val_gt_dir = '/home/riotu/Image_restoration/Hamad/copyright/dataset/iSAID-dataset/val/gt'\n",
        "    val_lq_dir = '/home/riotu/Image_restoration/Hamad/copyright/dataset/iSAID-dataset/val/low'\n",
        "    save_dir = '/home/riotu/Image_restoration/Hamad/copyright/dataset/iSAID-dataset/Results'\n",
        "    weights_path = '/home/riotu/Image_restoration/Hamad/copyright/inference/2_inference_code/Night_final_weight.pth' \n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Define transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((32, 32)),  # Adjust size as needed\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = ShadowRemovalDataset(train_gt_dir, train_lq_dir, transform=transform, augment=True)\n",
        "    val_dataset = ShadowRemovalDataset(val_gt_dir, val_lq_dir, transform=transform, augment=False)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
        "    print(\"Data loaders created successfully\")\n",
        "\n",
        "    # Initialize model\n",
        "    # Initialize model\n",
        "    # model = SSVA_Net(img_channel=3, width=32, middle_blk_num=1,\n",
        "    #                          enc_blk_nums=[1, 1, 1, 1], dec_blk_nums=[1, 1, 1, 1],\n",
        "    #                          d_state=64)  # Single_Branch\n",
        "    model = DualBranch_DBRSNet(img_channel=3,\n",
        "        width=32,\n",
        "        middle_blk_num=1,\n",
        "        enc_blk_nums=[1, 1, 1, 1],\n",
        "        dec_blk_nums=[1, 1, 1, 1],\n",
        "        d_state=64\n",
        "    )\n",
        "    print(\"Model initialized\")\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f'Total number of parameters: {total_params}')\n",
        "    #print(model)\n",
        "    # Load weights if available\n",
        "    if os.path.exists(weights_path):\n",
        "       model.load_state_dict(torch.load(weights_path, map_location=DEVICE))\n",
        "       print(f\"Loaded model weights from {weights_path}\")\n",
        "    else:\n",
        "       print(\"No pre-trained weights found, training from scratch.\")\n",
        "\n",
        "    # Train the model\n",
        "    train(model, train_loader, val_loader, NUM_EPOCHS, DEVICE, save_dir, LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVDDDiMz_z8Y"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XJvid0T_2uK",
        "outputId": "6dcad5d0-5f79-46c0-8e10-05658646ca28"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "from PIL import Image\n",
        "import os\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# Constants\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def load_model(model, weights_path):\n",
        "    \"\"\"\n",
        "    Load the model weights.\n",
        "    Args:\n",
        "        model: Initialized model object.\n",
        "        weights_path: Path to the pre-trained weights.\n",
        "    Returns:\n",
        "        model: Model with loaded weights.\n",
        "    \"\"\"\n",
        "    if os.path.exists(weights_path):\n",
        "        model.load_state_dict(torch.load(weights_path, map_location=DEVICE))\n",
        "        print(f\"Model weights loaded from {weights_path}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Model weights not found at {weights_path}\")\n",
        "    return model\n",
        "\n",
        "def preprocess_image(image_path, target_size=(512, 512)):\n",
        "    \"\"\"\n",
        "    Preprocess the input image for the model.\n",
        "    Args:\n",
        "        image_path: Path to the input image.\n",
        "        target_size: Tuple (H, W) for resizing.\n",
        "    Returns:\n",
        "        tensor: Preprocessed image tensor.\n",
        "    \"\"\"\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(target_size),\n",
        "        ToTensor()\n",
        "    ])\n",
        "    tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "    return tensor\n",
        "\n",
        "def save_output_image(tensor, output_path):\n",
        "    \"\"\"\n",
        "    Save the output tensor as an image.\n",
        "    Args:\n",
        "        tensor: Predicted image tensor.\n",
        "        output_path: Path to save the output image.\n",
        "    \"\"\"\n",
        "    image = ToPILImage()(tensor.squeeze(0).cpu())\n",
        "    image.save(output_path)\n",
        "    print(f\"Output saved to {output_path}\")\n",
        "\n",
        "def inference(model, input_image_path, output_image_path, target_size=(512, 512)):\n",
        "    \"\"\"\n",
        "    Run inference using the trained model.\n",
        "    Args:\n",
        "        model: Trained model.\n",
        "        input_image_path: Path to the input image.\n",
        "        output_image_path: Path to save the output image.\n",
        "        target_size: Tuple (H, W) for resizing the input.\n",
        "    \"\"\"\n",
        "    # Preprocess input image\n",
        "    input_tensor = preprocess_image(input_image_path, target_size=target_size).to(DEVICE)\n",
        "\n",
        "    # Perform inference\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output_tensor = model(input_tensor)\n",
        "\n",
        "    # Save the output\n",
        "    save_output_image(output_tensor, output_image_path)\n",
        "\n",
        "# Main execution for inference\n",
        "if __name__ == \"__main__\":\n",
        "    # Define paths\n",
        "    input_image_path = \"/home/riotu/Image_restoration/Hamad/copyright/dataset/iSAID-high-pixel/low/15/P0170.png\"  # Replace with your input image path\n",
        "    output_image_path = \"/content/sample_data/night_free_image.png\"  # Replace with your desired output path\n",
        "    model_weights_path = \"/home/riotu/Image_restoration/Hamad/copyright/inference/2_inference_code/Night_final_weight.pth\"  # Replace with your trained model weights\n",
        "\n",
        "    # Initialize and load the model\n",
        "    model = DualBranch_DBRSNet(img_channel=3,\n",
        "        width=32,\n",
        "        middle_blk_num=1,\n",
        "        enc_blk_nums=[1, 1, 1, 1],\n",
        "        dec_blk_nums=[1, 1, 1, 1],\n",
        "        d_state=64\n",
        "    )\n",
        "    model = load_model(model.to(DEVICE), model_weights_path)\n",
        "\n",
        "    # Run inference\n",
        "    inference(model, input_image_path, output_image_path, target_size=(512, 512))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "lmtexcH1BuhQ",
        "outputId": "5b2a272c-8c5f-4047-a810-6d7f3008e3b9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import ToPILImage\n",
        "\n",
        "def display_input_output(input_image_path, output_image_path):\n",
        "    \"\"\"\n",
        "    Display the input shadow image and the output shadow-free image side by side.\n",
        "    Args:\n",
        "        input_image_path: Path to the input shadow image.\n",
        "        output_image_path: Path to the output shadow-free image.\n",
        "    \"\"\"\n",
        "    # Load images\n",
        "    input_image = Image.open(input_image_path).convert(\"RGB\")\n",
        "    output_image = Image.open(output_image_path).convert(\"RGB\")\n",
        "\n",
        "    # Display images side by side\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Display shadow image\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(\"Shadow Image\")\n",
        "    plt.imshow(input_image)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # Display shadow-free image\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title(\"Shadow-Free Image\")\n",
        "    plt.imshow(output_image)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "input_image_path = \"/home/riotu/Image_restoration/Hamad/copyright/dataset/iSAID-high-pixel/low/15/P0170.png\"  # Replace with the same input image path used for inference\n",
        "output_image_path = \"/content/sample_data/night_free_image.png\"  # Replace with the path where the output image was saved\n",
        "\n",
        "display_input_output(input_image_path, output_image_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHbJQvWW7woh"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCvRr1fO7wEZ",
        "outputId": "567c8736-b47d-4345-c1f6-663d122ee3aa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import lpips\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as F\n",
        "from PIL import Image\n",
        "from imageio.v2 import imread\n",
        "import skimage\n",
        "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
        "from skimage.metrics import structural_similarity as compare_ssim\n",
        "from skimage.color import rgb2lab\n",
        "import scipy\n",
        "\n",
        "loss_fn_vgg = lpips.LPIPS(net='vgg').cuda() # vgg is used in the paper\n",
        "\n",
        "def load_item(gt_path, pre_path, mask_path):\n",
        "\n",
        "\n",
        "    gt = imread(gt_path)\n",
        "    try:\n",
        "        pre = imread(pre_path)\n",
        "    except:\n",
        "        pre = imread(pre_path.replace('.JPG', '.png'))\n",
        "    if mask_path is not None:\n",
        "        mask = imread(mask_path)\n",
        "\n",
        "\n",
        "    # resize to gt size\n",
        "    pre = resize(pre, (gt.shape[0], gt.shape[1]))\n",
        "    if mask_path is not None:\n",
        "        mask = resize(mask, (gt.shape[0], gt.shape[1]))\n",
        "        mask = (mask > 255 * 0.9).astype(np.uint8) * 255\n",
        "\n",
        "    if mask_path is not None:\n",
        "        return to_tensor(gt), to_tensor(pre), to_tensor(mask)\n",
        "    else:\n",
        "        return to_tensor(gt), to_tensor(pre), None\n",
        "\n",
        "\n",
        "def to_tensor(img):\n",
        "    img = Image.fromarray(img)\n",
        "    img_t = F.to_tensor(img).float()\n",
        "    img_t = img_t.unsqueeze(dim=0)\n",
        "    return img_t\n",
        "\n",
        "\n",
        "def resize(img, target_size):\n",
        "    img = skimage.transform.resize(img, target_size, mode='reflect', anti_aliasing=True)\n",
        "    img = (img * 255).astype(np.uint8)  # Ensure the image is in uint8 format\n",
        "\n",
        "    return img\n",
        "\n",
        "def calc_rmse(real_img, fake_img):\n",
        "    # Convert to LAB color space\n",
        "    real_lab = rgb2lab(real_img)\n",
        "    fake_lab = rgb2lab(fake_img)\n",
        "    rmse = np.sqrt(((real_lab - fake_lab) ** 2).mean())\n",
        "    return rmse\n",
        "\n",
        "\n",
        "def metric(gt, pre):\n",
        "    transf = torchvision.transforms.Compose(\n",
        "                [torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
        "\n",
        "    lpips_value = loss_fn_vgg(transf(pre[0]).cuda(), transf(gt[0]).cuda()).item()\n",
        "\n",
        "    pre = pre * 255.0\n",
        "    pre = pre.permute(0, 2, 3, 1)\n",
        "    pre = pre.detach().cpu().numpy().astype(np.uint8)[0]\n",
        "\n",
        "    gt = gt * 255.0\n",
        "    gt = gt.permute(0, 2, 3, 1)\n",
        "    gt = gt.cpu().detach().numpy().astype(np.uint8)[0]\n",
        "\n",
        "    psnr = compare_psnr(gt, pre)\n",
        "    ssim = compare_ssim(gt, pre, data_range=255, channel_axis=-1)\n",
        "    rmse = calc_rmse(gt, pre)\n",
        "\n",
        "    return psnr, ssim, lpips_value, rmse\n",
        "\n",
        "\n",
        "\n",
        "def evaluation(gt_root, pre_root, mask_root):\n",
        "    fnames = os.listdir(gt_root)\n",
        "    fnames.sort()\n",
        "\n",
        "    psnr_all_list, ssim_all_list, lpips_all_list, rmse_all_list = [], [], [], []\n",
        "    psnr_non_list, ssim_non_list, lpips_non_list, rmse_non_list = [], [], [], []\n",
        "    psnr_shadow_list, ssim_shadow_list, lpips_shadow_list, rmse_shadow_list = [], [], [], []\n",
        "\n",
        "    for fname in fnames:\n",
        "        gt_path = os.path.join(gt_root, fname)\n",
        "        pre_path = os.path.join(pre_root, fname)\n",
        "        if mask_root is not None:\n",
        "            mask_path = os.path.join(mask_root, fname)\n",
        "\n",
        "        # For SDR only, replace the mask path _free.jpg to .png\n",
        "        if mask_root is not None:\n",
        "            mask_path = mask_path.replace('.jpg', '.png')\n",
        "        else:\n",
        "            mask_path = None\n",
        "        pre_path = pre_path.replace('.jpg', '.png')\n",
        "        if not os.path.exists(pre_path):\n",
        "            pre_path = pre_path.replace('.png', '.jpg')\n",
        "\n",
        "\n",
        "        gt, pre, mask = load_item(gt_path, pre_path, mask_path)\n",
        "\n",
        "        psnr_all, ssim_all, lpips_all, rmse_all = metric(gt, pre)\n",
        "\n",
        "        psnr_all_list.append(psnr_all)\n",
        "        ssim_all_list.append(ssim_all)\n",
        "        lpips_all_list.append(lpips_all)\n",
        "        rmse_all_list.append(rmse_all)\n",
        "\n",
        "    print('-----------------------------------------------------------------------------')\n",
        "    print(f'All psnr: {round(np.average(psnr_all_list), 4)} ssim: {round(np.average(ssim_all_list), 4)} lpips: {round(np.average(lpips_all_list), 4)} rmse: {round(np.average(rmse_all_list), 4)}')\n",
        "\n",
        "\n",
        "########## Document Night Removal Evaluation ##########\n",
        "mask_root = None # There is no mask ground truth for document shadow removal dataset\n",
        "gt_root = '/home/riotu/Image_restoration/Hamad/copyright/dataset/iSAID-high-pixel/gt'\n",
        "pred_root = '/home/riotu/Image_restoration/Hamad/copyright/dataset/iSAID-high-pixel/out'\n",
        "\n",
        "# Start evaluation\n",
        "evaluation(gt_root, pred_root, mask_root)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "BarlqUux6eY6"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
