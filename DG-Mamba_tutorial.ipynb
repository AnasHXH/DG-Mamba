{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FSovHjCR2Q35",
        "outputId": "e7f0e501-f714-4794-9feb-f493b1080513"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision einops==0.3.2 tqdm==4.64.1 numpy==1.23.5 scikit-image==0.19.3 lpips==0.1.4 pyyaml==6.0 Pillow==9.4.0 matplotlib==3.6.2 opencv-python==4.6.0.66\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2XGRjOI5XOB"
      },
      "source": [
        "## Instructions to Access and Use the Datasets\n",
        "\n",
        "Open the provided dataset links:\n",
        "\n",
        "iSAID Dataset: https://drive.google.com/file/d/1mlTTdbqG1ZheaWsBcIjAKDyCdbuAqpvy/view\n",
        "\n",
        "darkrs Dataset : https://drive.google.com/file/d/1XQGpzB9vDGkO7ULnGOF86cyZdqtrX4tI/view\n",
        "\n",
        "LOL Dataset : https://drive.google.com/file/d/1L-kqSQyrmMueBh_ziWoPFhfsAh50h20H/view\n",
        "\n",
        "Exdark Dataset: https://github.com/cs-chan/Exclusively-Dark-Image-Dataset?tab=readme-ov-file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2zXsw912ocI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "from torchvision.utils import save_image\n",
        "from einops import rearrange\n",
        "from PIL import Image\n",
        "import os\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "# Constants\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import random\n",
        "\n",
        "class NightRemovalDataset(Dataset):\n",
        "    def __init__(self, gt_dir, lq_dir, transform=None, augment=False):\n",
        "        self.gt_dir = gt_dir\n",
        "        self.lq_dir = lq_dir\n",
        "        self.transform = transform\n",
        "        self.augment = augment\n",
        "        self.gt_images = sorted(os.listdir(gt_dir))\n",
        "        self.lq_images = sorted(os.listdir(lq_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.gt_images)\n",
        "\n",
        "    def augment_images(self, gt_image, lq_image):\n",
        "        # Random rotation\n",
        "        angle = random.choice([0, 90, 180, 270])\n",
        "        gt_image = TF.rotate(gt_image, angle)\n",
        "        lq_image = TF.rotate(lq_image, angle)\n",
        "\n",
        "        # Random horizontal flipping\n",
        "        if random.random() > 0.25:\n",
        "            gt_image = TF.hflip(gt_image)\n",
        "            lq_image = TF.hflip(lq_image)\n",
        "\n",
        "        # Random vertical flipping\n",
        "        if random.random() > 0.25:\n",
        "            gt_image = TF.vflip(gt_image)\n",
        "            lq_image = TF.vflip(lq_image)\n",
        "\n",
        "        return gt_image, lq_image\n",
        "\n",
        "    def random_crop(self, gt_image, lq_image):\n",
        "        i, j, h, w = transforms.RandomCrop.get_params(\n",
        "            gt_image, output_size=(256, 256))\n",
        "        gt_image = TF.crop(gt_image, i, j, h, w)\n",
        "        lq_image = TF.crop(lq_image, i, j, h, w)\n",
        "        return gt_image, lq_image\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        gt_img_path = os.path.join(self.gt_dir, self.gt_images[idx])\n",
        "        lq_img_path = os.path.join(self.lq_dir, self.lq_images[idx])\n",
        "\n",
        "        gt_image = Image.open(gt_img_path).convert(\"RGB\")\n",
        "        lq_image = Image.open(lq_img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.augment:\n",
        "            #pass\n",
        "            gt_image, lq_image = self.augment_images(gt_image, lq_image)\n",
        "            gt_image, lq_image = self.random_crop(gt_image, lq_image)\n",
        "\n",
        "        if self.transform:\n",
        "            gt_image = self.transform(gt_image)\n",
        "            lq_image = self.transform(lq_image)\n",
        "\n",
        "        return lq_image, gt_image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SD_3Dsiy3C7F"
      },
      "outputs": [],
      "source": [
        "# models/dg_mamba.py\n",
        "# DG-Mamba: Dual-Branch Gated Mamba for Nighttime Remote Sensing Image Enhancement\n",
        "# \n",
        "# Component Naming Convention:\n",
        "# - DGMamba: Dual-Branch Gated Mamba (full network)\n",
        "# - DGMambaBranch: Single branch of the dual-branch architecture\n",
        "# - GMB: Gated Mamba Block (outer block with dual residuals)\n",
        "# - CAGM: Context-Aware Gated Mixer (inner module)\n",
        "# - GSM: Gated Selective Modulation (content-adaptive gating)\n",
        "# - LFP: Learned Feature Projection (linear projection with positional embedding)\n",
        "# - MSCM: Multi-Scale Context Module (ASPP-based context aggregation)\n",
        "# - ElementWiseGating: Split-and-multiply gating (x1 * x2)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ElementWiseGating(nn.Module):\n",
        "    \"\"\"\n",
        "    Element-wise Gating mechanism.\n",
        "    \n",
        "    Splits input along channel dimension and computes element-wise product.\n",
        "    Widely used in efficient architectures like NAFNet.\n",
        "    \n",
        "    Operation: y = x1 ⊙ x2, where [x1, x2] = split(x)\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "        x1, x2 = x.chunk(2, dim=-1)\n",
        "        return x1 * x2\n",
        "\n",
        "\n",
        "class MSCM(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Scale Context Module (MSCM).\n",
        "    \n",
        "    Based on Atrous Spatial Pyramid Pooling (ASPP) for multi-scale \n",
        "    context aggregation. Uses dilated convolutions with rates {1, 6, 12, 18}\n",
        "    plus global average pooling to capture context at multiple scales.\n",
        "    \n",
        "    Args:\n",
        "        in_channels: Number of input channels\n",
        "        out_channels: Number of output channels\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(MSCM, self).__init__()\n",
        "        \n",
        "        # Multi-scale dilated convolutions\n",
        "        self.conv_d1 = nn.Conv2d(in_channels, out_channels, 1, bias=False)\n",
        "        self.conv_d6 = nn.Conv2d(in_channels, out_channels, 3, padding=6, dilation=6, bias=False)\n",
        "        self.conv_d12 = nn.Conv2d(in_channels, out_channels, 3, padding=12, dilation=12, bias=False)\n",
        "        self.conv_d18 = nn.Conv2d(in_channels, out_channels, 3, padding=18, dilation=18, bias=False)\n",
        "        \n",
        "        # Global context branch\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.conv_global = nn.Conv2d(in_channels, out_channels, 1, bias=False)\n",
        "        \n",
        "        # Fusion layer\n",
        "        self.conv_fuse = nn.Conv2d(out_channels * 5, out_channels, 1, bias=False)\n",
        "        self.norm = nn.LayerNorm(out_channels)\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape (B, C, H, W)\n",
        "            \n",
        "        Returns:\n",
        "            Multi-scale context features of shape (B, C, H, W)\n",
        "        \"\"\"\n",
        "        size = x.shape[-2:]\n",
        "        \n",
        "        # Multi-scale feature extraction\n",
        "        feat_d1 = self.conv_d1(x)\n",
        "        feat_d6 = self.conv_d6(x)\n",
        "        feat_d12 = self.conv_d12(x)\n",
        "        feat_d18 = self.conv_d18(x)\n",
        "        \n",
        "        # Global context\n",
        "        feat_global = self.conv_global(self.global_pool(x))\n",
        "        feat_global = F.interpolate(feat_global, size=size, mode='bilinear', align_corners=False)\n",
        "        \n",
        "        # Concatenate and fuse\n",
        "        out = torch.cat((feat_d1, feat_d6, feat_d12, feat_d18, feat_global), dim=1)\n",
        "        out = self.conv_fuse(out)\n",
        "        \n",
        "        # Normalize and activate\n",
        "        out = out.permute(0, 2, 3, 1)  # (B, C, H, W) → (B, H, W, C)\n",
        "        out = self.norm(out)\n",
        "        out = out.permute(0, 3, 1, 2)  # (B, H, W, C) → (B, C, H, W)\n",
        "        \n",
        "        return self.act(out)\n",
        "\n",
        "\n",
        "class LFP(nn.Module):\n",
        "    \"\"\"\n",
        "    Learned Feature Projection (LFP).\n",
        "    \n",
        "    Applies learned linear projections with positional embedding to modulate\n",
        "    features at each spatial location. Uses query-key-value formulation\n",
        "    for feature transformation.\n",
        "    \n",
        "    Args:\n",
        "        dim: Feature dimension (number of channels)\n",
        "    \"\"\"\n",
        "    def __init__(self, dim):\n",
        "        super(LFP, self).__init__()\n",
        "        self.dim = dim\n",
        "        \n",
        "        # Learnable projections\n",
        "        self.proj_q = nn.Linear(dim, dim)\n",
        "        self.proj_k = nn.Linear(dim, dim)\n",
        "        self.proj_v = nn.Linear(dim, dim)\n",
        "        \n",
        "        # Scaling factor\n",
        "        self.scale = dim ** -0.5\n",
        "        \n",
        "        # Learnable positional embedding\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, 1, 1, dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape (B, H, W, C)\n",
        "            \n",
        "        Returns:\n",
        "            Projected features of shape (B, H, W, C)\n",
        "        \"\"\"\n",
        "        B, H, W, C = x.shape\n",
        "        \n",
        "        # Add positional embedding\n",
        "        x = x + self.pos_embedding\n",
        "        x = x.view(B, H * W, C)\n",
        "        \n",
        "        # Compute projections\n",
        "        q = self.proj_q(x)  # (B, N, C)\n",
        "        k = self.proj_k(x)  # (B, N, C)\n",
        "        v = self.proj_v(x)  # (B, N, C)\n",
        "        \n",
        "        # Compute position-wise attention scores\n",
        "        q = q.view(B, H * W, 1, C)  # (B, N, 1, C)\n",
        "        k = k.view(B, H * W, C, 1)  # (B, N, C, 1)\n",
        "        attn = torch.matmul(q, k).squeeze(2) * self.scale  # (B, N, C)\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        \n",
        "        # Apply attention to values\n",
        "        out = attn * v\n",
        "        out = out.view(B, H, W, C)\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "class CAGM(nn.Module):\n",
        "    \"\"\"\n",
        "    Context-Aware Gated Mixer (CAGM).\n",
        "    \n",
        "    Core mixing module that combines:\n",
        "    - Path 1: GSM → SiLU-Gating → Element-wise Gating → LFP\n",
        "    - Path 2: MSCM (Multi-Scale Context Module)\n",
        "    \n",
        "    The two paths are fused via residual addition to integrate\n",
        "    content-adaptive gating with multi-scale spatial context.\n",
        "    \n",
        "    Args:\n",
        "        d_model: Model dimension (number of channels)\n",
        "        d_state: State dimension (unused, kept for compatibility)\n",
        "        d_conv: Convolution kernel size for depth-wise conv\n",
        "        expand: Channel expansion factor (default: 2.0)\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, d_state=16, d_conv=3, expand=2.0, \n",
        "                 dt_rank=64, dt_min=0.001, dt_max=0.1, dt_init=\"random\", dt_scale=1.0):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.d_state = d_state\n",
        "        self.d_conv = d_conv\n",
        "        self.expand = expand\n",
        "        self.d_inner = int(self.expand * self.d_model)  # 2 * d_model\n",
        "        self.dt_rank = dt_rank\n",
        "\n",
        "        # Input projection: d_model → 4 * d_model\n",
        "        self.in_proj = nn.Linear(self.d_model, self.d_inner * 2)\n",
        "        \n",
        "        # Depth-wise convolution\n",
        "        self.dwconv = nn.Conv2d(\n",
        "            self.d_inner, self.d_inner, \n",
        "            kernel_size=d_conv, \n",
        "            padding=(d_conv - 1) // 2, \n",
        "            groups=self.d_inner\n",
        "        )\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "        # GSM projections\n",
        "        self.gsm_proj = nn.Linear(self.d_inner, self.d_inner * 2)\n",
        "        self.delta_proj = nn.Linear(self.d_inner, self.d_inner)\n",
        "\n",
        "        # Normalization\n",
        "        self.out_norm = nn.LayerNorm(self.d_inner)\n",
        "\n",
        "        # Output projection\n",
        "        self.out_proj = nn.Linear(self.d_inner // 2, d_model)\n",
        "\n",
        "        # Sub-modules\n",
        "        self.element_gate = ElementWiseGating()\n",
        "        self.mscm = MSCM(d_model, d_model)\n",
        "        self.lfp = LFP(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape (B, H, W, C)\n",
        "            \n",
        "        Returns:\n",
        "            Output tensor of shape (B, H, W, C)\n",
        "        \"\"\"\n",
        "        B, H, W, C = x.shape\n",
        "\n",
        "        # ============ Path 2: Multi-Scale Context ============\n",
        "        x_mscm = self.mscm(x.permute(0, 3, 1, 2)).permute(0, 2, 3, 1)\n",
        "\n",
        "        # ============ Path 1: Gated Processing ============\n",
        "        \n",
        "        # Input projection and split\n",
        "        x_proj = self.in_proj(x)\n",
        "        x_main, z = x_proj.chunk(2, dim=-1)  # x_main: 2C, z: 2C (gating signal)\n",
        "        \n",
        "        # Depth-wise convolution\n",
        "        x_main = x_main.permute(0, 3, 1, 2)  # (B, H, W, C) → (B, C, H, W)\n",
        "        x_main = self.dwconv(x_main)\n",
        "        x_main = x_main.permute(0, 2, 3, 1)  # (B, C, H, W) → (B, H, W, C)\n",
        "        x_main = self.act(x_main)\n",
        "        \n",
        "        # Gated Selective Modulation (GSM)\n",
        "        y = self.gated_selective_modulation(x_main)\n",
        "        \n",
        "        # Layer normalization\n",
        "        y = self.out_norm(y)\n",
        "        \n",
        "        # SiLU-Gating: y = y ⊙ SiLU(z) = y ⊙ z ⊙ σ(z)\n",
        "        y = y * F.silu(z)\n",
        "\n",
        "        # Element-wise Gating: split and multiply\n",
        "        y = self.element_gate(y)\n",
        "\n",
        "        # Learned Feature Projection\n",
        "        y = self.lfp(y)\n",
        "\n",
        "        # ============ Path Fusion ============\n",
        "        y = y + x_mscm\n",
        "\n",
        "        # Output projection\n",
        "        out = self.out_proj(y)\n",
        "        \n",
        "        return out\n",
        "\n",
        "    def gated_selective_modulation(self, x):\n",
        "        \"\"\"\n",
        "        Gated Selective Modulation (GSM).\n",
        "        \n",
        "        Content-adaptive dual-path gating inspired by Mamba's selective mechanism.\n",
        "        Replaces sequential state-space recurrence with parallel gating.\n",
        "        \n",
        "        Formula: y = x ⊙ σ(Δ) + x_content ⊙ tanh(x_content)\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape (B, H, W, C)\n",
        "            \n",
        "        Returns:\n",
        "            Modulated features of shape (B, H, W, C)\n",
        "        \"\"\"\n",
        "        B, H, W, C = x.shape\n",
        "        \n",
        "        # Flatten spatial dimensions\n",
        "        x_flat = x.reshape(B, H * W, C)\n",
        "        \n",
        "        # Project to 2C dimensions\n",
        "        x_dbl = self.gsm_proj(x_flat)\n",
        "        x_dbl = x_dbl.view(B, H, W, -1)\n",
        "        \n",
        "        # Split into delta (gate) and content\n",
        "        delta_raw, x_content = x_dbl.chunk(2, dim=-1)\n",
        "        \n",
        "        # Compute gating coefficients: Δ = softplus(W_Δ · delta_raw)\n",
        "        delta = F.softplus(self.delta_proj(delta_raw))\n",
        "        \n",
        "        # Dual-path gating:\n",
        "        # - Path A: Input gated by learned delta → x ⊙ σ(Δ)\n",
        "        # - Path B: Self-gated content → x_content ⊙ tanh(x_content)\n",
        "        y = x * torch.sigmoid(delta) + x_content * torch.tanh(x_content)\n",
        "        \n",
        "        return y\n",
        "\n",
        "\n",
        "class GMB(nn.Module):\n",
        "    \"\"\"\n",
        "    Gated Mamba Block (GMB).\n",
        "    \n",
        "    Building block of DG-Mamba with dual-residual structure:\n",
        "    1. LayerNorm → CAGM → Residual Add\n",
        "    2. LayerNorm → Conv Block → Residual Add\n",
        "    \n",
        "    Args:\n",
        "        d_model: Model dimension (number of channels)\n",
        "        d_state: State dimension for CAGM\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, d_state=16):\n",
        "        super().__init__()\n",
        "        \n",
        "        # First normalization and CAGM\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.cagm = CAGM(d_model, d_state)\n",
        "        \n",
        "        # Second normalization and conv refinement\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.conv_refine = nn.Sequential(\n",
        "            nn.Conv2d(d_model, d_model, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(d_model, d_model, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass with dual residual connections.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape (B, H, W, C)\n",
        "            \n",
        "        Returns:\n",
        "            Output tensor of shape (B, H, W, C)\n",
        "        \"\"\"\n",
        "        # First residual: CAGM path\n",
        "        residual = x\n",
        "        x = self.norm1(x)\n",
        "        x = residual + self.cagm(x)\n",
        "        \n",
        "        # Second residual: Conv refinement path\n",
        "        residual = x\n",
        "        x = self.norm2(x)\n",
        "        x = x.permute(0, 3, 1, 2)  # (B, H, W, C) → (B, C, H, W)\n",
        "        x = self.conv_refine(x)\n",
        "        x = x.permute(0, 2, 3, 1)  # (B, C, H, W) → (B, H, W, C)\n",
        "        x = residual + x\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class DGMambaBranch(nn.Module):\n",
        "    \"\"\"\n",
        "    Single branch of DG-Mamba.\n",
        "    \n",
        "    U-Net style encoder-decoder architecture with GMB blocks.\n",
        "    Uses skip connections between encoder and decoder stages.\n",
        "    \n",
        "    Args:\n",
        "        img_channel: Number of input image channels (default: 3)\n",
        "        width: Base channel width (default: 32)\n",
        "        middle_blk_num: Number of GMB blocks in bottleneck (default: 1)\n",
        "        enc_blk_nums: Number of GMB blocks per encoder stage (default: [1,1,1,1])\n",
        "        dec_blk_nums: Number of GMB blocks per decoder stage (default: [1,1,1,1])\n",
        "        d_state: State dimension for CAGM (default: 64)\n",
        "    \"\"\"\n",
        "    def __init__(self, img_channel=3, width=32, middle_blk_num=1, \n",
        "                 enc_blk_nums=[1, 1, 1, 1], dec_blk_nums=[1, 1, 1, 1], d_state=64):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Input projection\n",
        "        self.intro = nn.Conv2d(\n",
        "            img_channel, width, \n",
        "            kernel_size=3, padding=1, stride=1, \n",
        "            groups=1, bias=True\n",
        "        )\n",
        "        \n",
        "        # Output projection\n",
        "        self.ending = nn.Conv2d(\n",
        "            width, img_channel, \n",
        "            kernel_size=3, padding=1, stride=1, \n",
        "            groups=1, bias=True\n",
        "        )\n",
        "\n",
        "        # Build encoder, decoder, and bottleneck\n",
        "        self.encoders = nn.ModuleList()\n",
        "        self.decoders = nn.ModuleList()\n",
        "        self.middle_blks = nn.ModuleList()\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "\n",
        "        chan = width\n",
        "        \n",
        "        # Encoder stages\n",
        "        for num in enc_blk_nums:\n",
        "            self.encoders.append(\n",
        "                nn.Sequential(*[GMB(chan, d_state) for _ in range(num)])\n",
        "            )\n",
        "            self.downs.append(nn.Conv2d(chan, 2 * chan, kernel_size=2, stride=2))\n",
        "            chan = chan * 2\n",
        "\n",
        "        # Bottleneck\n",
        "        self.middle_blks = nn.Sequential(\n",
        "            *[GMB(chan, d_state) for _ in range(middle_blk_num)]\n",
        "        )\n",
        "\n",
        "        # Decoder stages\n",
        "        for num in dec_blk_nums:\n",
        "            self.ups.append(nn.Sequential(\n",
        "                nn.Conv2d(chan, chan * 2, kernel_size=1, bias=False),\n",
        "                nn.PixelShuffle(2)\n",
        "            ))\n",
        "            chan = chan // 2\n",
        "            self.decoders.append(\n",
        "                nn.Sequential(*[GMB(chan, d_state) for _ in range(num)])\n",
        "            )\n",
        "\n",
        "        # Padding size for input alignment\n",
        "        self.padder_size = 2 ** len(self.encoders)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        \n",
        "        Args:\n",
        "            inp: Input image of shape (B, C, H, W)\n",
        "            \n",
        "        Returns:\n",
        "            Enhanced image of shape (B, C, H, W)\n",
        "        \"\"\"\n",
        "        B, C, H, W = inp.shape\n",
        "        \n",
        "        # Pad input to be divisible by padder_size\n",
        "        inp = self.check_image_size(inp)\n",
        "        \n",
        "        # Input projection\n",
        "        x = self.intro(inp)\n",
        "        x = x.permute(0, 2, 3, 1)  # (B, C, H, W) → (B, H, W, C)\n",
        "\n",
        "        # Encoder with skip connections\n",
        "        enc_features = []\n",
        "        for encoder, down in zip(self.encoders, self.downs):\n",
        "            x = encoder(x)\n",
        "            enc_features.append(x)\n",
        "            x = x.permute(0, 3, 1, 2)  # (B, H, W, C) → (B, C, H, W)\n",
        "            x = down(x)\n",
        "            x = x.permute(0, 2, 3, 1)  # (B, C, H, W) → (B, H, W, C)\n",
        "\n",
        "        # Bottleneck\n",
        "        x = self.middle_blks(x)\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        for decoder, up, enc_skip in zip(self.decoders, self.ups, enc_features[::-1]):\n",
        "            x = x.permute(0, 3, 1, 2)  # (B, H, W, C) → (B, C, H, W)\n",
        "            x = up(x)\n",
        "            x = x.permute(0, 2, 3, 1)  # (B, C, H, W) → (B, H, W, C)\n",
        "            x = x + enc_skip  # Skip connection\n",
        "            x = decoder(x)\n",
        "\n",
        "        # Output projection with global residual\n",
        "        x = x.permute(0, 3, 1, 2)  # (B, H, W, C) → (B, C, H, W)\n",
        "        x = self.ending(x)\n",
        "        x = x + inp  # Global residual connection\n",
        "\n",
        "        # Crop to original size\n",
        "        return x[:, :, :H, :W]\n",
        "\n",
        "    def check_image_size(self, x):\n",
        "        \"\"\"Pad image to be divisible by padder_size.\"\"\"\n",
        "        _, _, h, w = x.size()\n",
        "        mod_pad_h = (self.padder_size - h % self.padder_size) % self.padder_size\n",
        "        mod_pad_w = (self.padder_size - w % self.padder_size) % self.padder_size\n",
        "        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h))\n",
        "        return x\n",
        "\n",
        "\n",
        "class DGMamba(nn.Module):\n",
        "    \"\"\"\n",
        "    DG-Mamba: Dual-Branch Gated Mamba.\n",
        "    \n",
        "    Dual-branch architecture for nighttime remote sensing image enhancement.\n",
        "    Two parallel DGMambaBranch networks process the input independently,\n",
        "    and their outputs are fused via a learned combination layer.\n",
        "    \n",
        "    Architecture:\n",
        "        Input → [Branch A] → Output A ─┐\n",
        "                                       ├→ Concat → Conv1x1 → Sigmoid → Output\n",
        "        Input → [Branch B] → Output B ─┘\n",
        "    \n",
        "    Args:\n",
        "        img_channel: Number of input image channels (default: 3)\n",
        "        width: Base channel width for each branch (default: 32)\n",
        "        middle_blk_num: Number of GMB blocks in bottleneck (default: 1)\n",
        "        enc_blk_nums: Number of GMB blocks per encoder stage (default: [1,1,1,1])\n",
        "        dec_blk_nums: Number of GMB blocks per decoder stage (default: [1,1,1,1])\n",
        "        d_state: State dimension for CAGM (default: 64)\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_channel=3,\n",
        "        width=32,\n",
        "        middle_blk_num=1,\n",
        "        enc_blk_nums=[1, 1, 1, 1],\n",
        "        dec_blk_nums=[1, 1, 1, 1],\n",
        "        d_state=64\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Branch A\n",
        "        self.branch_a = DGMambaBranch(\n",
        "            img_channel=img_channel,\n",
        "            width=width,\n",
        "            middle_blk_num=middle_blk_num,\n",
        "            enc_blk_nums=enc_blk_nums,\n",
        "            dec_blk_nums=dec_blk_nums,\n",
        "            d_state=d_state\n",
        "        )\n",
        "\n",
        "        # Branch B\n",
        "        self.branch_b = DGMambaBranch(\n",
        "            img_channel=img_channel,\n",
        "            width=width,\n",
        "            middle_blk_num=middle_blk_num,\n",
        "            enc_blk_nums=enc_blk_nums,\n",
        "            dec_blk_nums=dec_blk_nums,\n",
        "            d_state=d_state\n",
        "        )\n",
        "\n",
        "        # Fusion layer: Concatenate + Conv1x1 + Sigmoid\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Conv2d(img_channel * 2, img_channel, kernel_size=1, stride=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        \n",
        "        Args:\n",
        "            x: Input image of shape (B, C, H, W)\n",
        "            \n",
        "        Returns:\n",
        "            Enhanced image of shape (B, C, H, W)\n",
        "        \"\"\"\n",
        "        # Process through both branches\n",
        "        out_a = self.branch_a(x)  # (B, C, H, W)\n",
        "        out_b = self.branch_b(x)  # (B, C, H, W)\n",
        "\n",
        "        # Fuse branch outputs\n",
        "        fused = torch.cat([out_a, out_b], dim=1)  # (B, 2C, H, W)\n",
        "        fused = self.fusion(fused)                 # (B, C, H, W)\n",
        "\n",
        "        return fused"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUiQVPES6PX0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Loss functions (unchanged)\n",
        "class VGGLoss(nn.Module):\n",
        "    def __init__(self, device=DEVICE):\n",
        "        super(VGGLoss, self).__init__()\n",
        "        vgg = models.vgg19(pretrained=True).features.to(device).eval()\n",
        "        self.vgg_layers = nn.Sequential(*list(vgg)[:36]).to(device)\n",
        "        self.device = device\n",
        "        for param in self.vgg_layers.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = x.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "        x_vgg = self.vgg_layers(x)\n",
        "        y_vgg = self.vgg_layers(y)\n",
        "        return nn.functional.l1_loss(x_vgg, y_vgg)\n",
        "\n",
        "class FocalFrequencyLoss(nn.Module):\n",
        "    def __init__(self, alpha=1.0, device=DEVICE):\n",
        "        super(FocalFrequencyLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        input = input.to(self.device)\n",
        "        target = target.to(self.device)\n",
        "        input_fft = torch.fft.fft2(input)\n",
        "        target_fft = torch.fft.fft2(target)\n",
        "        diff = input_fft - target_fft\n",
        "        abs_diff = torch.abs(diff)\n",
        "        loss = torch.pow(abs_diff, self.alpha)\n",
        "        return torch.mean(loss)\n",
        "\n",
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, lambda_vgg=0.01, lambda_ff=0.1, device=DEVICE):\n",
        "        super(CombinedLoss, self).__init__()\n",
        "        self.lambda_vgg = lambda_vgg\n",
        "        self.lambda_ff = lambda_ff\n",
        "        self.l1_loss = nn.L1Loss()\n",
        "        self.vgg_loss = VGGLoss(device=device)\n",
        "        self.ff_loss = FocalFrequencyLoss(device=device)\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        l1 = self.l1_loss(input, target)\n",
        "        vgg = self.vgg_loss(input, target)\n",
        "        ff = self.ff_loss(input, target)\n",
        "        return l1 + self.lambda_vgg * vgg + self.lambda_ff * ff\n",
        "\n",
        "class VGG_comLoss(nn.Module):\n",
        "    def __init__(self, lambda_vgg=0.01, lambda_ff=0.1, device=DEVICE):\n",
        "        super(VGG_comLoss, self).__init__()\n",
        "        self.lambda_vgg = lambda_vgg\n",
        "        self.lambda_ff = lambda_ff\n",
        "        self.l1_loss = nn.L1Loss()\n",
        "        self.vgg_loss = VGGLoss(device=device)\n",
        "        self.ff_loss = FocalFrequencyLoss(device=device)\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        l1 = self.l1_loss(input, target)\n",
        "        vgg = self.vgg_loss(input, target)\n",
        "        ff = self.ff_loss(input, target)\n",
        "        return l1 + self.lambda_vgg * vgg\n",
        "\n",
        "class CharbonnierLoss(nn.Module):\n",
        "    \"\"\"Charbonnier Loss (L1)\"\"\"\n",
        "\n",
        "    def __init__(self, eps=1e-3):\n",
        "        super(CharbonnierLoss, self).__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        diff = x - y\n",
        "        # loss = torch.sum(torch.sqrt(diff * diff + self.eps))\n",
        "        loss = torch.mean(torch.sqrt((diff * diff) + (self.eps*self.eps)))\n",
        "        return loss\n",
        "# l1_loss = nn.L1Loss()\n",
        "# Training and validation functions (unchanged)\n",
        "\n",
        "\n",
        "\n",
        "def train(model, train_loader, val_loader, num_epochs, device, save_dir, LEARNING_RATE):\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999))\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-8)\n",
        "    criterion = VGG_comLoss(device= device)\n",
        "\n",
        "    # Initialize lists to store metrics\n",
        "    training_losses = []\n",
        "    validation_losses = []\n",
        "    psnr_list = []\n",
        "    ssim_list = []\n",
        "    lpips_list = []\n",
        "    psnr_basicsr_list = []\n",
        "    ssim_basicsr_list = []\n",
        "\n",
        "    # Initialize best PSNR\n",
        "    best_psnr = 0.0\n",
        "    # best_lpips = 100.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{NUM_EPOCHS}', unit='batch') as pbar:\n",
        "            for real_shadow, real_free in train_loader:\n",
        "                real_shadow, real_free = real_shadow.to(device), real_free.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                pred_free = model(real_shadow)\n",
        "                loss = criterion(pred_free, real_free)\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                pbar.set_postfix(loss=loss.item())\n",
        "                pbar.update(1)\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        training_losses.append(avg_loss)\n",
        "        print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}] - Average Loss: {avg_loss:.4f}\")\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validate every epoch\n",
        "        avg_val_loss, avg_psnr, avg_ssim, avg_lpips, avg_psnr_basicsr, avg_ssim_basicsr = validate_and_save(\n",
        "            model, val_loader, criterion, device, epoch, save_dir, num_epochs\n",
        "        )\n",
        "\n",
        "        validation_losses.append(avg_val_loss)\n",
        "        psnr_list.append(avg_psnr)\n",
        "        ssim_list.append(avg_ssim)\n",
        "        lpips_list.append(avg_lpips)\n",
        "        psnr_basicsr_list.append(avg_psnr_basicsr)\n",
        "        ssim_basicsr_list.append(avg_ssim_basicsr)\n",
        "\n",
        "        # Check if current avg_psnr is better than best_psnr\n",
        "        if avg_psnr > best_psnr:\n",
        "        # if avg_lpips < best_lpips:\n",
        "            best_psnr = avg_psnr\n",
        "            # best_lpips = avg_lpips\n",
        "            # Save model checkpoint\n",
        "            checkpoint_dir = os.path.join(save_dir, 'best_checkpoints')\n",
        "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "            checkpoint_path = os.path.join(checkpoint_dir, f'shadow_removal_best.pth')\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            print(f\"New best PSNR: {best_psnr:.4f}. Model saved to {checkpoint_path}\")\n",
        "        else:\n",
        "            print(f\"PSNR did not improve from {best_psnr:.4f}\")\n",
        "\n",
        "\n",
        "    # After training, save metrics to CSV\n",
        "    import pandas as pd\n",
        "    metrics_data = {\n",
        "        'Epoch': list(range(1, num_epochs + 1)),\n",
        "        'Training Loss': training_losses,\n",
        "        'Validation Loss': validation_losses,\n",
        "        'PSNR': psnr_list,\n",
        "        'SSIM': ssim_list,\n",
        "        'LPIPS': lpips_list,\n",
        "        'PSNR Basicsr': psnr_basicsr_list,\n",
        "        'SSIM Basicsr': ssim_basicsr_list\n",
        "    }\n",
        "    metrics_df = pd.DataFrame(metrics_data)\n",
        "    csv_path = os.path.join(save_dir, 'training_metrics.csv')\n",
        "    metrics_df.to_csv(csv_path, index=False)\n",
        "    print(f\"Metrics saved to {csv_path}\")\n",
        "\n",
        "    # Plot the curves\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Plot Training and Validation Loss\n",
        "    plt.figure()\n",
        "    plt.plot(metrics_df['Epoch'], metrics_df['Training Loss'], label='Training Loss')\n",
        "    plt.plot(metrics_df['Epoch'], metrics_df['Validation Loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(save_dir, 'loss_curve.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Plot PSNR\n",
        "    plt.figure()\n",
        "    plt.plot(metrics_df['Epoch'], metrics_df['PSNR'], label='PSNR (RGB)')\n",
        "    plt.plot(metrics_df['Epoch'], metrics_df['PSNR Basicsr'], label='PSNR Basicsr (Y channel)')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('PSNR')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(save_dir, 'psnr_curve.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Plot SSIM\n",
        "    plt.figure()\n",
        "    plt.plot(metrics_df['Epoch'], metrics_df['SSIM'], label='SSIM (RGB)')\n",
        "    plt.plot(metrics_df['Epoch'], metrics_df['SSIM Basicsr'], label='SSIM Basicsr (Y channel)')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('SSIM')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(save_dir, 'ssim_curve.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Plot LPIPS\n",
        "    plt.figure()\n",
        "    plt.plot(metrics_df['Epoch'], metrics_df['LPIPS'], label='LPIPS')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('LPIPS')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(save_dir, 'lpips_curve.png'))\n",
        "    plt.close()\n",
        "\n",
        "    print(\"Training completed\")\n",
        "\n",
        "\n",
        "####################################################\n",
        "# Validation\n",
        "# %cd /data/Image_restoration/shadow_removal_cvpr_2025/my_model/window/BasicSR\n",
        "import lpips\n",
        "import torch\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "# from basicsr.metrics.psnr_ssim import calculate_psnr_pt, calculate_ssim_pt\n",
        "# from basicsr.utils import img2tensor\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import numpy as np\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "def validate_and_save(model, val_loader, criterion, device, epoch, save_dir, num_epochs):\n",
        "    model.eval()\n",
        "    total_psnr = 0\n",
        "    total_ssim = 0\n",
        "    total_psnr_basicsr = 0\n",
        "    total_ssim_basicsr = 0\n",
        "    total_lpips = 0\n",
        "    val_loss = 0\n",
        "    total_num_images = 0  # Keep track of total images processed\n",
        "\n",
        "    # Initialize LPIPS model\n",
        "    lpips_model = lpips.LPIPS(net='alex').to(device)\n",
        "\n",
        "    # Create a separate folder for checkpoints\n",
        "    checkpoint_dir = os.path.join(save_dir, 'checkpoints')\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    sample_images_dir = os.path.join(save_dir, 'sample_images')\n",
        "    os.makedirs(sample_images_dir, exist_ok=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (real_shadow, real_free) in enumerate(val_loader):\n",
        "            real_shadow, real_free = real_shadow.to(device), real_free.to(device)\n",
        "            pred_free = model(real_shadow)\n",
        "\n",
        "            loss = criterion(pred_free, real_free)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            batch_size = real_free.size(0)\n",
        "            total_num_images += batch_size\n",
        "\n",
        "            # Calculate PSNR and SSIM using original method (on RGB images)\n",
        "            pred_free_np = pred_free.cpu().numpy().transpose(0, 2, 3, 1)\n",
        "            real_free_np = real_free.cpu().numpy().transpose(0, 2, 3, 1)\n",
        "\n",
        "            for j in range(batch_size):\n",
        "                # Original PSNR and SSIM (RGB images)\n",
        "                psnr_value = psnr(real_free_np[j], pred_free_np[j], data_range=1)\n",
        "                ssim_value = ssim(real_free_np[j], pred_free_np[j], data_range=1, channel_axis=-1)\n",
        "                total_psnr += psnr_value\n",
        "                total_ssim += ssim_value\n",
        "\n",
        "            # Calculate PSNR and SSIM using basicsr (on tensors)\n",
        "            psnr_basicsr = calculate_psnr_pt(pred_free, real_free, crop_border=4, test_y_channel=True)\n",
        "            ssim_basicsr = calculate_ssim_pt(pred_free, real_free, crop_border=4, test_y_channel=True)\n",
        "            total_psnr_basicsr += psnr_basicsr.sum().item()\n",
        "            total_ssim_basicsr += ssim_basicsr.sum().item()\n",
        "\n",
        "            # Calculate LPIPS\n",
        "            lpips_score = lpips_model(pred_free, real_free)\n",
        "            total_lpips += lpips_score.sum().item()\n",
        "\n",
        "            # Save some sample images\n",
        "            if i == 0:\n",
        "                save_image(pred_free, os.path.join(sample_images_dir, f'pred_free_epoch_{epoch + 1}.png'))\n",
        "                save_image(real_shadow, os.path.join(sample_images_dir, f'real_shadow_epoch_{epoch + 1}.png'))\n",
        "                save_image(real_free, os.path.join(sample_images_dir, f'real_free_epoch_{epoch + 1}.png'))\n",
        "\n",
        "    avg_psnr = total_psnr / total_num_images\n",
        "    avg_ssim = total_ssim / total_num_images\n",
        "    avg_psnr_basicsr = total_psnr_basicsr / total_num_images\n",
        "    avg_ssim_basicsr = total_ssim_basicsr / total_num_images\n",
        "    avg_lpips = total_lpips / total_num_images\n",
        "    avg_loss = val_loss / len(val_loader)\n",
        "\n",
        "    print(f\"Validation after Epoch [{epoch + 1}/{num_epochs}]\")\n",
        "    print(f\"Avg Loss: {avg_loss:.4f}\")\n",
        "    print(f\"Original PSNR (RGB): {avg_psnr:.4f}, Original SSIM (RGB): {avg_ssim:.4f}\")\n",
        "    print(f\"Basicsr PSNR (Y channel, crop=4): {avg_psnr_basicsr:.4f}, Basicsr SSIM (Y channel, crop=4): {avg_ssim_basicsr:.4f}\")\n",
        "    print(f\"Avg LPIPS: {avg_lpips:.4f}\")\n",
        "\n",
        "    # Save model checkpoint in the separate folder\n",
        "    #checkpoint_path = os.path.join(checkpoint_dir, f'shadow_removal_epoch_{epoch + 1}.pth')\n",
        "    #torch.save(model.state_dict(), checkpoint_path)\n",
        "    #print(f\"Checkpoint saved to {checkpoint_path}\")\n",
        "\n",
        "    return avg_loss, avg_psnr, avg_ssim, avg_lpips, avg_psnr_basicsr, avg_ssim_basicsr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BarlqUux6eY6"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fhue4BWm6ZSL"
      },
      "outputs": [],
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Define directories\n",
        "    LEARNING_RATE = 0.0002\n",
        "    NUM_EPOCHS = 500\n",
        "    BATCH_SIZE = 1\n",
        "    SAVE_INTERVAL = 10\n",
        "\n",
        "    train_gt_dir = '/home/riotu/Image_restoration/Hamad/copyright/dataset/iSAID-dataset/train/'\n",
        "    train_lq_dir = '/home/riotu/Image_restoration/Hamad/copyright/dataset/iSAID-dataset/train/low'\n",
        "    val_gt_dir = '/home/riotu/Image_restoration/Hamad/copyright/dataset/iSAID-dataset/val/gt'\n",
        "    val_lq_dir = '/home/riotu/Image_restoration/Hamad/copyright/dataset/iSAID-dataset/val/low'\n",
        "    save_dir = '/home/riotu/Image_restoration/Hamad/copyright/dataset/iSAID-dataset/Results'\n",
        "    weights_path = '/home/riotu/Image_restoration/Hamad/copyright/inference/2_inference_code/Night_final_weight.pth' \n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Define transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((32, 32)),  # Adjust size as needed\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = NightRemovalDataset(train_gt_dir, train_lq_dir, transform=transform, augment=True)\n",
        "    val_dataset = NightRemovalDataset(val_gt_dir, val_lq_dir, transform=transform, augment=False)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
        "    print(\"Data loaders created successfully\")\n",
        "\n",
        "    # Initialize model\n",
        "    # Initialize model\n",
        "    # model = SSVA_Net(img_channel=3, width=32, middle_blk_num=1,\n",
        "    #                          enc_blk_nums=[1, 1, 1, 1], dec_blk_nums=[1, 1, 1, 1],\n",
        "    #                          d_state=64)  # Single_Branch\n",
        "    model = DGMamba(img_channel=3,\n",
        "        width=32,\n",
        "        middle_blk_num=1,\n",
        "        enc_blk_nums=[1, 1, 1, 1],\n",
        "        dec_blk_nums=[1, 1, 1, 1],\n",
        "        d_state=64\n",
        "    )\n",
        "    print(\"Model initialized\")\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f'Total number of parameters: {total_params}')\n",
        "    #print(model)\n",
        "    # Load weights if available\n",
        "    if os.path.exists(weights_path):\n",
        "       model.load_state_dict(torch.load(weights_path, map_location=DEVICE))\n",
        "       print(f\"Loaded model weights from {weights_path}\")\n",
        "    else:\n",
        "       print(\"No pre-trained weights found, training from scratch.\")\n",
        "\n",
        "    # Train the model\n",
        "    train(model, train_loader, val_loader, NUM_EPOCHS, DEVICE, save_dir, LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVDDDiMz_z8Y"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XJvid0T_2uK",
        "outputId": "6dcad5d0-5f79-46c0-8e10-05658646ca28"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "from PIL import Image\n",
        "import os\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# Constants\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def load_model(model, weights_path):\n",
        "    \"\"\"\n",
        "    Load the model weights.\n",
        "    Args:\n",
        "        model: Initialized model object.\n",
        "        weights_path: Path to the pre-trained weights.\n",
        "    Returns:\n",
        "        model: Model with loaded weights.\n",
        "    \"\"\"\n",
        "    if os.path.exists(weights_path):\n",
        "        model.load_state_dict(torch.load(weights_path, map_location=DEVICE))\n",
        "        print(f\"Model weights loaded from {weights_path}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Model weights not found at {weights_path}\")\n",
        "    return model\n",
        "\n",
        "def preprocess_image(image_path, target_size=(512, 512)):\n",
        "    \"\"\"\n",
        "    Preprocess the input image for the model.\n",
        "    Args:\n",
        "        image_path: Path to the input image.\n",
        "        target_size: Tuple (H, W) for resizing.\n",
        "    Returns:\n",
        "        tensor: Preprocessed image tensor.\n",
        "    \"\"\"\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(target_size),\n",
        "        ToTensor()\n",
        "    ])\n",
        "    tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "    return tensor\n",
        "\n",
        "def save_output_image(tensor, output_path):\n",
        "    \"\"\"\n",
        "    Save the output tensor as an image.\n",
        "    Args:\n",
        "        tensor: Predicted image tensor.\n",
        "        output_path: Path to save the output image.\n",
        "    \"\"\"\n",
        "    image = ToPILImage()(tensor.squeeze(0).cpu())\n",
        "    image.save(output_path)\n",
        "    print(f\"Output saved to {output_path}\")\n",
        "\n",
        "def inference(model, input_image_path, output_image_path, target_size=(512, 512)):\n",
        "    \"\"\"\n",
        "    Run inference using the trained model.\n",
        "    Args:\n",
        "        model: Trained model.\n",
        "        input_image_path: Path to the input image.\n",
        "        output_image_path: Path to save the output image.\n",
        "        target_size: Tuple (H, W) for resizing the input.\n",
        "    \"\"\"\n",
        "    # Preprocess input image\n",
        "    input_tensor = preprocess_image(input_image_path, target_size=target_size).to(DEVICE)\n",
        "\n",
        "    # Perform inference\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output_tensor = model(input_tensor)\n",
        "\n",
        "    # Save the output\n",
        "    save_output_image(output_tensor, output_image_path)\n",
        "\n",
        "# Main execution for inference\n",
        "if __name__ == \"__main__\":\n",
        "    # Define paths\n",
        "    input_image_path = \"/home/riotu/Image_restoration/Hamad/copyright/dataset/iSAID-high-pixel/low/15/P0170.png\"  # Replace with your input image path\n",
        "    output_image_path = \"/content/sample_data/night_free_image.png\"  # Replace with your desired output path\n",
        "    model_weights_path = \"/home/riotu/Image_restoration/Hamad/copyright/inference/2_inference_code/Night_final_weight.pth\"  # Replace with your trained model weights\n",
        "\n",
        "    # Initialize and load the model\n",
        "    model = DGMamba(img_channel=3,\n",
        "        width=32,\n",
        "        middle_blk_num=1,\n",
        "        enc_blk_nums=[1, 1, 1, 1],\n",
        "        dec_blk_nums=[1, 1, 1, 1],\n",
        "        d_state=64\n",
        "    )\n",
        "    model = load_model(model.to(DEVICE), model_weights_path)\n",
        "\n",
        "    # Run inference\n",
        "    inference(model, input_image_path, output_image_path, target_size=(512, 512))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "lmtexcH1BuhQ",
        "outputId": "5b2a272c-8c5f-4047-a810-6d7f3008e3b9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import ToPILImage\n",
        "\n",
        "def display_input_output(input_image_path, output_image_path):\n",
        "    \"\"\"\n",
        "    Display the input shadow image and the output shadow-free image side by side.\n",
        "    Args:\n",
        "        input_image_path: Path to the input shadow image.\n",
        "        output_image_path: Path to the output shadow-free image.\n",
        "    \"\"\"\n",
        "    # Load images\n",
        "    input_image = Image.open(input_image_path).convert(\"RGB\")\n",
        "    output_image = Image.open(output_image_path).convert(\"RGB\")\n",
        "\n",
        "    # Display images side by side\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Display shadow image\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(\"Shadow Image\")\n",
        "    plt.imshow(input_image)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # Display shadow-free image\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title(\"Shadow-Free Image\")\n",
        "    plt.imshow(output_image)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "input_image_path = \"/home/riotu/Image_restoration/Hamad/copyright/dataset/iSAID-high-pixel/low/15/P0170.png\"  # Replace with the same input image path used for inference\n",
        "output_image_path = \"/content/sample_data/night_free_image.png\"  # Replace with the path where the output image was saved\n",
        "\n",
        "display_input_output(input_image_path, output_image_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHbJQvWW7woh"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCvRr1fO7wEZ",
        "outputId": "567c8736-b47d-4345-c1f6-663d122ee3aa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import lpips\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as F\n",
        "from PIL import Image\n",
        "from imageio.v2 import imread\n",
        "import skimage\n",
        "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
        "from skimage.metrics import structural_similarity as compare_ssim\n",
        "from skimage.color import rgb2lab\n",
        "import scipy\n",
        "\n",
        "loss_fn_vgg = lpips.LPIPS(net='vgg').cuda() # vgg is used in the paper\n",
        "\n",
        "def load_item(gt_path, pre_path, mask_path):\n",
        "\n",
        "\n",
        "    gt = imread(gt_path)\n",
        "    try:\n",
        "        pre = imread(pre_path)\n",
        "    except:\n",
        "        pre = imread(pre_path.replace('.JPG', '.png'))\n",
        "    if mask_path is not None:\n",
        "        mask = imread(mask_path)\n",
        "\n",
        "\n",
        "    # resize to gt size\n",
        "    pre = resize(pre, (gt.shape[0], gt.shape[1]))\n",
        "    if mask_path is not None:\n",
        "        mask = resize(mask, (gt.shape[0], gt.shape[1]))\n",
        "        mask = (mask > 255 * 0.9).astype(np.uint8) * 255\n",
        "\n",
        "    if mask_path is not None:\n",
        "        return to_tensor(gt), to_tensor(pre), to_tensor(mask)\n",
        "    else:\n",
        "        return to_tensor(gt), to_tensor(pre), None\n",
        "\n",
        "\n",
        "def to_tensor(img):\n",
        "    img = Image.fromarray(img)\n",
        "    img_t = F.to_tensor(img).float()\n",
        "    img_t = img_t.unsqueeze(dim=0)\n",
        "    return img_t\n",
        "\n",
        "\n",
        "def resize(img, target_size):\n",
        "    img = skimage.transform.resize(img, target_size, mode='reflect', anti_aliasing=True)\n",
        "    img = (img * 255).astype(np.uint8)  # Ensure the image is in uint8 format\n",
        "\n",
        "    return img\n",
        "\n",
        "def calc_rmse(real_img, fake_img):\n",
        "    # Convert to LAB color space\n",
        "    real_lab = rgb2lab(real_img)\n",
        "    fake_lab = rgb2lab(fake_img)\n",
        "    rmse = np.sqrt(((real_lab - fake_lab) ** 2).mean())\n",
        "    return rmse\n",
        "\n",
        "\n",
        "def metric(gt, pre):\n",
        "    transf = torchvision.transforms.Compose(\n",
        "                [torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
        "\n",
        "    lpips_value = loss_fn_vgg(transf(pre[0]).cuda(), transf(gt[0]).cuda()).item()\n",
        "\n",
        "    pre = pre * 255.0\n",
        "    pre = pre.permute(0, 2, 3, 1)\n",
        "    pre = pre.detach().cpu().numpy().astype(np.uint8)[0]\n",
        "\n",
        "    gt = gt * 255.0\n",
        "    gt = gt.permute(0, 2, 3, 1)\n",
        "    gt = gt.cpu().detach().numpy().astype(np.uint8)[0]\n",
        "\n",
        "    psnr = compare_psnr(gt, pre)\n",
        "    ssim = compare_ssim(gt, pre, data_range=255, channel_axis=-1)\n",
        "    rmse = calc_rmse(gt, pre)\n",
        "\n",
        "    return psnr, ssim, lpips_value, rmse\n",
        "\n",
        "\n",
        "\n",
        "def evaluation(gt_root, pre_root, mask_root):\n",
        "    fnames = os.listdir(gt_root)\n",
        "    fnames.sort()\n",
        "\n",
        "    psnr_all_list, ssim_all_list, lpips_all_list, rmse_all_list = [], [], [], []\n",
        "    psnr_non_list, ssim_non_list, lpips_non_list, rmse_non_list = [], [], [], []\n",
        "    psnr_shadow_list, ssim_shadow_list, lpips_shadow_list, rmse_shadow_list = [], [], [], []\n",
        "\n",
        "    for fname in fnames:\n",
        "        gt_path = os.path.join(gt_root, fname)\n",
        "        pre_path = os.path.join(pre_root, fname)\n",
        "        if mask_root is not None:\n",
        "            mask_path = os.path.join(mask_root, fname)\n",
        "\n",
        "        # For SDR only, replace the mask path _free.jpg to .png\n",
        "        if mask_root is not None:\n",
        "            mask_path = mask_path.replace('.jpg', '.png')\n",
        "        else:\n",
        "            mask_path = None\n",
        "        pre_path = pre_path.replace('.jpg', '.png')\n",
        "        if not os.path.exists(pre_path):\n",
        "            pre_path = pre_path.replace('.png', '.jpg')\n",
        "\n",
        "\n",
        "        gt, pre, mask = load_item(gt_path, pre_path, mask_path)\n",
        "\n",
        "        psnr_all, ssim_all, lpips_all, rmse_all = metric(gt, pre)\n",
        "\n",
        "        psnr_all_list.append(psnr_all)\n",
        "        ssim_all_list.append(ssim_all)\n",
        "        lpips_all_list.append(lpips_all)\n",
        "        rmse_all_list.append(rmse_all)\n",
        "\n",
        "    print('-----------------------------------------------------------------------------')\n",
        "    print(f'All psnr: {round(np.average(psnr_all_list), 4)} ssim: {round(np.average(ssim_all_list), 4)} lpips: {round(np.average(lpips_all_list), 4)} rmse: {round(np.average(rmse_all_list), 4)}')\n",
        "\n",
        "\n",
        "########## Document Night Removal Evaluation ##########\n",
        "mask_root = None # There is no mask ground truth for document shadow removal dataset\n",
        "gt_root = '/home/riotu/Image_restoration/Hamad/copyright/dataset/iSAID-high-pixel/gt'\n",
        "pred_root = '/home/riotu/Image_restoration/Hamad/copyright/dataset/iSAID-high-pixel/out'\n",
        "\n",
        "# Start evaluation\n",
        "evaluation(gt_root, pred_root, mask_root)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "BarlqUux6eY6"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
